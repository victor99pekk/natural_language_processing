{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CSE 156 PA1, Part 1\n",
        "\n"
      ],
      "metadata": {
        "id": "nhqcDPz2Nf-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSE 156 PA1, PyTorch Basics (25 points)\n",
        "\n",
        "\n",
        "### <font color='blue'> Due:  October 18, 2024 at  10pm </font>\n",
        "\n",
        "###### IMPORTANT: After copying this notebook to your Google Drive, paste a link to it below. To get a publicly-accessible link, click the *Share* button at the top right, then click \"Get shareable link\" and copy the link.\n",
        "#### <font color=\"red\">Link: </font>\n",
        "\n",
        "https://colab.research.google.com/drive/1VC_k4_pjtwhP8u947O2N_1Z_y_JIlfu1?usp=sharing\n",
        "\n",
        "---\n",
        "**Notes:**\n",
        "\n",
        "Make sure to save the notebook as you go along.\n",
        "\n",
        "Submission instructions are located at the bottom of the notebook.\n",
        "\n",
        "The code should run fairly quickly (a couple of minutes at most even without a GPU), if it takes much longer than that, its likely that you have introduced an error."
      ],
      "metadata": {
        "id": "_hvgvtdgtSGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: PyTorch Basics (25 Points)\n",
        "\n",
        "We will use PyTorch, a machine learning framework for the rest of the course. The first part of this assigment focuses on  PyTorch and how it is used for NLP.\n",
        "If you are new to [PyTorch](https://pytorch.org), it is highly recommended to work through  [the Stanford PyTorch Tutorial ](https://colab.research.google.com/drive/1Pz8b_h-W9zIBk1p2e6v-YFYThG1NkYeS?usp=sharing#scrollTo=u0ukr7quvrMx)"
      ],
      "metadata": {
        "id": "BG91IFlPOTE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 1.1 (2.5 points)\n",
        "\n",
        "In state-of-the-art NLP, words are represented by low-dimensional vectors,  referred to as *embeddings*. When processing sequences such as sentences, movie, reviews, or entire paragraphs,   word embeddings are used to compute a vector representation of the sequence,  denoted by $\\boldsymbol{x}$. In the cell below, the embeddings for the words in the sequence \"I like NLP\" are provided. Your task is to combine these embeddings into a single vector representation $\\boldsymbol{x}$, using  [element-wise vector addition](https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#elementwise-operations). This method is a simple way to obtain a sequence representation, namely, it is a *continuous bag-of-words (BoW) representation* of a sequence."
      ],
      "metadata": {
        "id": "WCVZCyANT-sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "# Seed the random number generator for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "input_sequence = 'I like NLP'\n",
        "\n",
        "# Initialize an embedding matrix\n",
        "# We have a vocabulary of 5 words, each represented by a 10-dimensional embedding vector.\n",
        "embeddings = torch.nn.Embedding(num_embeddings=5, embedding_dim=10)\n",
        "vocab = {'I': 0, 'like': 1, 'NLP': 2, 'classifiers': 3, '.': 4}\n",
        "\n",
        "\n",
        "# Convert the words to integer indices. These indices will be used to\n",
        "# retrieve the corresponding embeddings from the embedding matrix.\n",
        "# In PyTorch, operations are performed on Tensor objects, so we need to  convert\n",
        "# the list of indices to a LongTensor.\n",
        "indices = torch.LongTensor([vocab[w] for w in input_sequence.split()])\n",
        "\n",
        "input_sequence_embs = embeddings(indices)\n",
        "#print(embeddings(torch.LongTensor([1])))\n",
        "\n",
        "print('sequence embedding tensor size: ', input_sequence_embs.size())\n",
        "\n",
        "# The input_sequence_embs tensor contains the embeddings for each word in the input sequence.\n",
        "# The next step is to aggregate these embeddings into a single vector representation.\n",
        "# You will use  element-wise addition to do this.\n",
        "# Write the code to add the embeddings element-wise and store the result in the variable \"x\".\n",
        "\n",
        "print(input_sequence_embs)\n",
        "### YOUR CODE HERE!\n",
        "# Replace with the actual computation\n",
        "x = torch.sum(input_sequence_embs, dim=0)  # Sum along the first dimension\n",
        "\n",
        "### DO NOT MODIFY THE LINE BELOW\n",
        "print('input sequence embedding sum (continuous BoW): ', x)\n"
      ],
      "metadata": {
        "id": "HRgCs8TuWFKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6665cb9-c005-49a4-fe9e-01c3b313c878"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequence embedding tensor size:  torch.Size([3, 10])\n",
            "tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,\n",
            "          0.3223, -1.2633],\n",
            "        [ 0.3500,  0.3081,  0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959,\n",
            "          0.5667,  0.7935],\n",
            "        [ 0.5988, -1.5551, -0.3414,  1.8530,  0.7502, -0.5855, -0.1734,  0.1835,\n",
            "          1.3894,  1.5863]], grad_fn=<EmbeddingBackward0>)\n",
            "input sequence embedding sum (continuous BoW):  tensor([-0.1770, -2.3993, -0.4721,  2.6568,  2.7157, -0.1408, -1.8421, -3.6277,\n",
            "         2.2783,  1.1165], grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 1.2 (2.5 points)\n",
        "Element-wise addition is not the best way to aggregate individual word embeddings in a sequence into a single vector representation (a process known as *composition*). State one significant limitation of using element-wise addition as a composition function for a sequence of word embeddings corresponding to a sentence.\n",
        "---"
      ],
      "metadata": {
        "id": "WdajAoKGVByR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer:\n",
        "There are infinite ways of adding different elements to get the same result. This means if we have a sentence with word a and word b, and we then add them together to get result c. It is possible that we have another sentence with word x,y and z that also add up to result c even though word x, y and z mean something completely different from a and b\n"
      ],
      "metadata": {
        "id": "7sbir0XVVGN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 1.3 (5 points)\n",
        "The [softmax function](https://pytorch.org/docs/master/nn.functional.html#softmax) is used in nearly all the neural network architectures we will look at in this course. The softmax is computed on an $n$-dimensional vector $<x_1, x_2, \\dots, x_n>$ as $\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{1 \\leq j \\leq n} e^{x_j}}$. Given the   sequence representation $\\boldsymbol{x}$ we just computed, we can use the softmax function in combination with a linear projection using a matrix $W$ to transform $\\boldsymbol{x}$ into a probability distribution $p$ over the next word, expressed as $p = \\text{softmax}(W\\boldsymbol{x})$. Let us look at this in the cell below:"
      ],
      "metadata": {
        "id": "z115ktL2VQwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a random matrix W of size 10x5. This will serve as the weight matrix\n",
        "# for the linear projection of the vector x into a 5-dimensional space.\n",
        "W = torch.rand(10, 5)\n",
        "\n",
        "\n",
        "# Project the vector x to a 5-dimensional space using the matrix W. This projection is achieved through\n",
        "# matrix multiplication. After the projection, apply the softmax function to the result,\n",
        "# which converts the 5-dimensional projected vector into a probability distribution.\n",
        "# You can find the softmax function in PyTorch's  API (torch.nn.functional.softmax).\n",
        "# Store the resulting probability distribution in the variable \"probs\".\n",
        "\n",
        "### YOUR CODE HERE\n",
        "# Replace with the actual computation\n",
        "projected_x = torch.matmul(x, W)\n",
        "probs = torch.nn.functional.softmax(projected_x, dim=0) # This should be replaced with the actual computation\n",
        "\n",
        "\n",
        "### DO NOT MODIFY THE BELOW LINE!\n",
        "print('probability distribution', probs)\n"
      ],
      "metadata": {
        "id": "yvLFHHd7ab9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4816be22-97b5-4486-81a1-71cde7d7834d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability distribution tensor([0.0718, 0.0998, 0.1331, 0.6762, 0.0191], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 1.4 (10 points)\n",
        "\n",
        "In the example so far, we focused on a single sequence (\"I like NLP\"). However, in practical applications, itâ€™s common to process multiple sequences simultaneously. This practice, known as *batching*, allows for more efficient use of GPU parallelism. In batching, each sequence is considered an example within a larger batch\n",
        "\n",
        "For this question, you will  redo the previous computation, but with a batch of two sequences instead of just one. The final output of this cell should be a 2x5 matrix, where each row represents a probability distribution for a sequence. **Important: Avoid using loops in your solution, as you will lose points**. The code should be fully vectorized."
      ],
      "metadata": {
        "id": "ljIukDbfVgfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# For this example, we replicate our previous sequence indices to create a simple batch.\n",
        "# Normally, each example in the batch would be different.\n",
        "batch_indices = torch.cat(2 * [indices]).reshape((2, 3))\n",
        "batch_embs = embeddings(batch_indices)\n",
        "print('Batch embedding tensor size: ', batch_embs.size())\n",
        "\n",
        "# To process the batch, follow these steps:\n",
        "# Step 1: Aggregate the embeddings for each example in the batch into a single representation.\n",
        "# This is done through element-wise addition. Use torch.sum with the appropriate 'dim' argument\n",
        "# to sum across the sequence length (not the batch dimension).\n",
        "aggregated_batch_embs = torch.sum(batch_embs, dim=1)\n",
        "\n",
        "# Step 2: Project each aggregated representation into a 5-dimensional space using the matrix W.\n",
        "# This involves matrix multiplication, ensuring the resulting batch has the shape 2x5.\n",
        "W = torch.rand(10, 5)\n",
        "\n",
        "# Step 3: Apply the softmax function to the projected representations to obtain probability distributions.\n",
        "# Each row in the output matrix should sum to 1, representing a probability distribution for each batch example.\n",
        "projected_batch = torch.matmul(aggregated_batch_embs, W)\n",
        "\n",
        "### YOUR CODE HERE\n",
        "# Replace with the actual computation\n",
        "batch_probs = F.softmax(projected_batch, dim=1)\n",
        "\n",
        "### DO NOT MODIFY THE BELOW LINE\n",
        "print(\"Batch probability distributions:\", batch_probs)\n"
      ],
      "metadata": {
        "id": "64xEd2GIetE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e268cd7-0ce3-4b68-c310-4a0bf7418f54"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch embedding tensor size:  torch.Size([2, 3, 10])\n",
            "Batch probability distributions: tensor([[0.0655, 0.0028, 0.0023, 0.1176, 0.8118],\n",
            "        [0.0655, 0.0028, 0.0023, 0.1176, 0.8118]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Question 1.5 (5 points)\n",
        "\n",
        "When processing a text sequence, how should the system handle words that are not present in the existing vocabulary? In the current implementation, the presence of such out-of-vocabulary words causes the code to fail, as in the cell below. To address this issue,  a simple solution is to use the special token `<UNK>`,  added to the vocabulary to serve as a placeholder for any unknown words.\n",
        "\n",
        "Modify the indexing function to ensure that it checks each word against the known vocabulary and substitutes any out-of-vocabulary words with the `<UNK>` token.  Make sure not to add  any new words  to the vocabulary  except for the `<UNK>` token. Don't forget to adjust the embedding table.\n"
      ],
      "metadata": {
        "id": "l98pYQ2biukl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "# Seed the random number generator for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "input_sequence = 'I like linear'\n",
        "\n",
        "\n",
        "# Initialize an embedding matrix\n",
        "# We have a vocabulary of 5 words, each represented by a 10-dimensional embedding vector.\n",
        "embeddings = torch.nn.Embedding(num_embeddings=6, embedding_dim=10)\n",
        "vocab = {'I': 0, 'like': 1, 'NLP': 2, 'classifiers': 3, '.': 4, '<UNK>': 5}\n",
        "\n",
        "\n",
        "#indices = torch.LongTensor([vocab[w] for w in input_sequence.split()]) ### MODIFY THIS INDEXING\n",
        "\n",
        "indices = torch.LongTensor([vocab.get(w, vocab['<UNK>']) for w in input_sequence.split()])\n",
        "\n",
        "input_sequence_embs = embeddings(indices)\n",
        "print('sequence embedding tensor size: ', input_sequence_embs.size())\n",
        "\n"
      ],
      "metadata": {
        "id": "_LjvbU82is9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3981ecf-d956-402f-81a6-3b02a7e3e328"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequence embedding tensor size:  torch.Size([3, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------"
      ],
      "metadata": {
        "id": "FwX54r1DXQ-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------\n",
        "# <font color=\"blue\"> Submission Instructions</font>\n",
        "---------------------------\n",
        "\n",
        "1. Click the Save button at the top of the Jupyter Notebook.\n",
        "2. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of all cells).\n",
        "2. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
        "3. Once you've rerun everything, convert the notebook to PDF, you can use tools such as [nbconvert](https://nbconvert.readthedocs.io/en/latest/usage.html), which requires first downloading the  ipynb  to your local machine, and then running \"nbconvert\" . (If you have trouble using nbconvert, you can also save the webpage as pdf. <font color='blue'> Make sure all your solutions  are displayed in the pdf</font>, it's okay if the provided codes get cut off because lines are not wrapped in code cells).\n",
        "4. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing your graders will see!\n",
        "5. Submit your PDF on Gradescope, along with Parts 2 and 3 of the Assignment as described in the PA handhout.\n"
      ],
      "metadata": {
        "id": "YfuoIQajhtQd"
      }
    }
  ]
}